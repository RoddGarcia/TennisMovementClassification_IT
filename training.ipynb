{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['KeyPoints HPE'](KeyPoints_HPE.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe forehand: 69 vídeos\n",
      "Classe backhand: 46 vídeos\n",
      "Classe nenhum: 57 vídeos\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"Novos Renderizados 2025\"\n",
    "LABELS = [\"forehand\", \"backhand\", \"nenhum\"]\n",
    "\n",
    "for categoria in LABELS:\n",
    "    pasta_videos = os.path.join(DATASET_PATH, categoria)\n",
    "    num_videos = len(os.listdir(pasta_videos))\n",
    "    print(f\"Classe {categoria}: {num_videos} vídeos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Extraindo landmarks dos vídeos...\n",
      "Pesos das classes definidos manualmente: {0: 1.5, 1: 1.0, 2: 2.0}\n",
      "Landmarks extraídos! Total de amostras: 172\n",
      "Treinando o modelo LSTM...\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rodri\\Documents\\Codigos\\IT_Folder\\.tennis_it\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - accuracy: 0.3104 - loss: 1.6128 - val_accuracy: 0.3429 - val_loss: 1.1226\n",
      "Epoch 2/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3033 - loss: 1.5656 - val_accuracy: 0.3429 - val_loss: 1.1330\n",
      "Epoch 3/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.4002 - loss: 1.5456 - val_accuracy: 0.6000 - val_loss: 1.0544\n",
      "Epoch 4/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5142 - loss: 1.5187 - val_accuracy: 0.5714 - val_loss: 0.9671\n",
      "Epoch 5/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5953 - loss: 1.4277 - val_accuracy: 0.6857 - val_loss: 0.7516\n",
      "Epoch 6/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6219 - loss: 1.2193 - val_accuracy: 0.6857 - val_loss: 0.5946\n",
      "Epoch 7/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6420 - loss: 1.1776 - val_accuracy: 0.7429 - val_loss: 0.6406\n",
      "Epoch 8/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6535 - loss: 1.0095 - val_accuracy: 0.6857 - val_loss: 0.6495\n",
      "Epoch 9/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6044 - loss: 1.1272 - val_accuracy: 0.7143 - val_loss: 0.5917\n",
      "Epoch 10/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7176 - loss: 0.8353 - val_accuracy: 0.7143 - val_loss: 0.6323\n",
      "Epoch 11/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6717 - loss: 1.0216 - val_accuracy: 0.7143 - val_loss: 0.5717\n",
      "Epoch 12/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.6427 - loss: 0.8777 - val_accuracy: 0.7429 - val_loss: 0.5048\n",
      "Epoch 13/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.6314 - loss: 0.8913 - val_accuracy: 0.7143 - val_loss: 0.5512\n",
      "Epoch 14/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7060 - loss: 0.7804 - val_accuracy: 0.7714 - val_loss: 0.4729\n",
      "Epoch 15/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6943 - loss: 0.7511 - val_accuracy: 0.8000 - val_loss: 0.5178\n",
      "Epoch 16/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7558 - loss: 0.7090 - val_accuracy: 0.8857 - val_loss: 0.4699\n",
      "Epoch 17/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8247 - loss: 0.5829 - val_accuracy: 0.7714 - val_loss: 0.5247\n",
      "Epoch 18/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6501 - loss: 1.1122 - val_accuracy: 0.8286 - val_loss: 0.6975\n",
      "Epoch 19/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7301 - loss: 0.8354 - val_accuracy: 0.8857 - val_loss: 0.4763\n",
      "Epoch 20/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7961 - loss: 0.8537 - val_accuracy: 0.7714 - val_loss: 0.4786\n",
      "Epoch 21/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8528 - loss: 0.6559 - val_accuracy: 0.7714 - val_loss: 0.4987\n",
      "Epoch 22/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7554 - loss: 0.8017 - val_accuracy: 0.8857 - val_loss: 0.4395\n",
      "Epoch 23/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7965 - loss: 0.6060 - val_accuracy: 0.8857 - val_loss: 0.4760\n",
      "Epoch 24/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8089 - loss: 0.6628 - val_accuracy: 0.8857 - val_loss: 0.3993\n",
      "Epoch 25/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8081 - loss: 0.5343 - val_accuracy: 0.8857 - val_loss: 0.3827\n",
      "Epoch 26/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8415 - loss: 0.5654 - val_accuracy: 0.8286 - val_loss: 0.4861\n",
      "Epoch 27/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8223 - loss: 0.6250 - val_accuracy: 0.8571 - val_loss: 0.4470\n",
      "Epoch 28/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8082 - loss: 0.6228 - val_accuracy: 0.8857 - val_loss: 0.3784\n",
      "Epoch 29/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8513 - loss: 0.4582 - val_accuracy: 0.6857 - val_loss: 0.7979\n",
      "Epoch 30/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8349 - loss: 0.7504 - val_accuracy: 0.8571 - val_loss: 0.4268\n",
      "Epoch 31/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7936 - loss: 0.6696 - val_accuracy: 0.8571 - val_loss: 0.3711\n",
      "Epoch 32/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8241 - loss: 0.6105 - val_accuracy: 0.7429 - val_loss: 0.5953\n",
      "Epoch 33/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8532 - loss: 0.5440 - val_accuracy: 0.8857 - val_loss: 0.3205\n",
      "Epoch 34/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8376 - loss: 0.5641 - val_accuracy: 0.8857 - val_loss: 0.2995\n",
      "Epoch 35/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8464 - loss: 0.4847 - val_accuracy: 0.9143 - val_loss: 0.2962\n",
      "Epoch 36/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9014 - loss: 0.3604 - val_accuracy: 0.9143 - val_loss: 0.3287\n",
      "Epoch 37/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8660 - loss: 0.4164 - val_accuracy: 0.8857 - val_loss: 0.3528\n",
      "Epoch 38/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8409 - loss: 0.4514 - val_accuracy: 0.8857 - val_loss: 0.3596\n",
      "Epoch 39/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8854 - loss: 0.3613 - val_accuracy: 0.8857 - val_loss: 0.3366\n",
      "Epoch 40/40\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8578 - loss: 0.4780 - val_accuracy: 0.8857 - val_loss: 0.3094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8821 - loss: 0.3169\n",
      "Modelo treinado e salvo\n",
      "Acurácia final: 88.57%\n"
     ]
    }
   ],
   "source": [
    "#Inicializando o MediaPipe\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "db_path = \"Novos Renderizados 2025\"\n",
    "mov_labels = {\"forehand\": 0, \"backhand\": 1, \"nenhum\": 2}\n",
    "\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "print(\"Extraindo landmarks dos vídeos...\")\n",
    "\n",
    "braços_landmarks = [11, 12, 13, 14, 15, 16]  \n",
    "\n",
    "#Para cada item no diretório:\n",
    "for categoria, label in mov_labels.items():\n",
    "    pasta_videos = os.path.join(db_path, categoria)\n",
    "\n",
    "    for video_nome in os.listdir(pasta_videos):\n",
    "        video_path = os.path.join(pasta_videos, video_nome)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        landmarks_seq = []\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(frame_rgb)\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                frame_landmarks = []\n",
    "                for i in braços_landmarks:  #Apenas os pontos dos braços\n",
    "                    lm = results.pose_landmarks.landmark[i]\n",
    "                    frame_landmarks.extend([lm.x, lm.y, lm.z])  \n",
    "\n",
    "                landmarks_seq.append(frame_landmarks)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        janela = 30\n",
    "        if len(landmarks_seq) < janela:\n",
    "            padding = [[0] * (len(braços_landmarks) * 3)] * (janela - len(landmarks_seq))\n",
    "            landmarks_seq = padding + landmarks_seq\n",
    "        else:\n",
    "            landmarks_seq = landmarks_seq[-janela:]\n",
    "\n",
    "        X_data.append(landmarks_seq)\n",
    "        y_data.append(label)\n",
    "\n",
    "X_data = np.array(X_data)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "y_data = tf.keras.utils.to_categorical(y_data, num_classes=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "#Ajuste dos pesos para balanceamento (caso necessário)\n",
    "class_weights = {0: 1.5, 1: 1.0, 2: 2.0}\n",
    "\n",
    "print(\"Pesos das classes definidos manualmente:\", class_weights)\n",
    "\n",
    "print(f\"Foi possível extrair os landmarks com sucesso.\\nTotal de amostras: {len(X_data)}\\n\")\n",
    "print(\"Treinando o LSTM...\")\n",
    "\n",
    "#Criando o modelo LSTM\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(30, len(braços_landmarks) * 3)),\n",
    "    LSTM(64),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=32, validation_data=(X_test, y_test), class_weight=class_weights)\n",
    "\n",
    "model.save(\"modelo_tenis_multiclasse_v2.h5\")\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Modelo treinado e salvo\")\n",
    "print(f\"Acurácia final: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tennis_it",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
